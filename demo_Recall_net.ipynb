{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from data_process import *\n",
    "from Model        import *\n",
    "from ops          import _weight_variable\n",
    "\n",
    "BATCH_START    = 0\n",
    "BATCH_SIZE     = 50\n",
    "TIME_STEP      = 20\n",
    "LR             = 0.0001\n",
    "NUM_SPEAKER    = 77\n",
    "EPOCH_UP       = 50\n",
    "EPOCH_LOW      = 30\n",
    "SPEAKERIDX     = 0\n",
    "INPUT_SIZE     = 513\n",
    "OUTPUT_SIZE    = 513\n",
    "Mem_size       = 16\n",
    "WINDOW         = 1\n",
    "TOTAL_SIZE     = 0\n",
    "model_name = 'Master_Slave2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2127389908\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Model(INPUT_SIZE, OUTPUT_SIZE, BATCH_SIZE, TIME_STEP, LR, sequence_length=[TIME_STEP]*BATCH_SIZE, window=WINDOW)\n",
    "init  = tf.global_variables_initializer()\n",
    "sess  = tf.Session()\n",
    "sess.run(init)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, './Models/Master_Slave2'+str(NUM_SPEAKER)+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data done!     time:  1262.55739903\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mix_train, target1, target2, sequence_length = get_data_train(\n",
    "    '../mann/Train/Mix/', '../mann/Train/Target1/', '../mann/Train/Target2/', NUM_SPEAKER, BATCH_SIZE, TIME_STEP, longest=200)\n",
    "end = time.time()\n",
    "#np.save('./Train/mix.npz', mix)\n",
    "#np.save('./Train/t1.npz', t1)\n",
    "#np.save('./Train/t2npz', t2)\n",
    "print'Get data done!', \"    time: \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  0.718173241229  Encoder Loss:  0.668369286166  Decoder Loss:  0.767977197628  Training Time:  1926.0\n",
      "Epoch:  1  Training Loss:  0.508680267376  Encoder Loss:  0.514038774966  Decoder Loss:  0.503321760745  Training Time:  1926.0\n",
      "Epoch:  2  Training Loss:  0.444484774359  Encoder Loss:  0.457256689325  Decoder Loss:  0.431712858878  Training Time:  1926.0\n",
      "Epoch:  3  Training Loss:  0.401199498388  Encoder Loss:  0.417912168387  Decoder Loss:  0.384486828095  Training Time:  1925.0\n",
      "Epoch:  4  Training Loss:  0.368379492539  Encoder Loss:  0.38654420524  Decoder Loss:  0.350214780101  Training Time:  1925.0\n",
      "Epoch:  5  Training Loss:  0.346231710363  Encoder Loss:  0.366814972482  Decoder Loss:  0.325648448494  Training Time:  1926.0\n",
      "Epoch:  6  Training Loss:  0.328894733726  Encoder Loss:  0.351474965071  Decoder Loss:  0.306314502212  Training Time:  1926.0\n",
      "Epoch:  7  Training Loss:  0.311964064407  Encoder Loss:  0.335546499204  Decoder Loss:  0.288381630192  Training Time:  1925.0\n",
      "Epoch:  8  Training Loss:  0.29925784475  Encoder Loss:  0.324084508392  Decoder Loss:  0.274431180549  Training Time:  1925.0\n",
      "Epoch:  9  Training Loss:  0.2882411332  Encoder Loss:  0.313772858575  Decoder Loss:  0.262709406951  Training Time:  1926.0\n",
      "Epoch:  10  Training Loss:  0.276169978747  Encoder Loss:  0.302607654221  Decoder Loss:  0.249732303302  Training Time:  1925.0\n",
      "Epoch:  11  Training Loss:  0.268303855995  Encoder Loss:  0.296109026263  Decoder Loss:  0.240498685344  Training Time:  1925.0\n",
      "Epoch:  12  Training Loss:  0.26071817505  Encoder Loss:  0.289328194404  Decoder Loss:  0.232108155655  Training Time:  1926.0\n",
      "Epoch:  13  Training Loss:  0.253588174706  Encoder Loss:  0.282345753718  Decoder Loss:  0.224830595674  Training Time:  1925.0\n",
      "Epoch:  14  Training Loss:  0.247428937323  Encoder Loss:  0.276864355392  Decoder Loss:  0.217993519164  Training Time:  1926.0\n",
      "Epoch:  15  Training Loss:  0.242075530508  Encoder Loss:  0.272187215654  Decoder Loss:  0.211963845195  Training Time:  1925.0\n",
      "Epoch:  16  Training Loss:  0.236745381196  Encoder Loss:  0.266708502896  Decoder Loss:  0.206782259374  Training Time:  1925.0\n",
      "Epoch:  17  Training Loss:  0.232119155639  Encoder Loss:  0.262360886885  Decoder Loss:  0.201877425179  Training Time:  1925.0\n",
      "Epoch:  18  Training Loss:  0.228021871833  Encoder Loss:  0.258456207065  Decoder Loss:  0.197587536162  Training Time:  1926.0\n",
      "Epoch:  19  Training Loss:  0.223273319119  Encoder Loss:  0.253814209152  Decoder Loss:  0.192732429  Training Time:  1925.0\n",
      "Epoch:  20  Training Loss:  0.220269702788  Encoder Loss:  0.251055198472  Decoder Loss:  0.18948420692  Training Time:  1926.0\n",
      "Epoch:  21  Training Loss:  0.216535057  Encoder Loss:  0.247476930071  Decoder Loss:  0.185593183908  Training Time:  1925.0\n",
      "Epoch:  22  Training Loss:  0.213259171143  Encoder Loss:  0.24415578122  Decoder Loss:  0.182362560971  Training Time:  1925.0\n",
      "Epoch:  23  Training Loss:  0.21009205676  Encoder Loss:  0.241016011455  Decoder Loss:  0.179168102087  Training Time:  1925.0\n",
      "Epoch:  24  Training Loss:  0.207607335863  Encoder Loss:  0.238600981356  Decoder Loss:  0.176613690443  Training Time:  1925.0\n",
      "Epoch:  25  Training Loss:  0.205222182725  Encoder Loss:  0.236348639287  Decoder Loss:  0.174095725929  Training Time:  1925.0\n",
      "Epoch:  26  Training Loss:  0.202471030626  Encoder Loss:  0.233440752528  Decoder Loss:  0.171501308588  Training Time:  1925.0\n",
      "Epoch:  27  Training Loss:  0.200289303274  Encoder Loss:  0.231488375368  Decoder Loss:  0.169090231336  Training Time:  1926.0\n",
      "Epoch:  28  Training Loss:  0.19857535262  Encoder Loss:  0.229832864295  Decoder Loss:  0.167317840706  Training Time:  1925.0\n",
      "Epoch:  29  Training Loss:  0.196060161182  Encoder Loss:  0.227180842508  Decoder Loss:  0.164939479943  Training Time:  1926.0\n",
      "Epoch:  30  Training Loss:  0.194291172191  Encoder Loss:  0.225181903661  Decoder Loss:  0.163400441008  Training Time:  1925.0\n",
      "Epoch:  31  Training Loss:  0.19223276106  Encoder Loss:  0.223211557313  Decoder Loss:  0.161253965105  Training Time:  1925.0\n",
      "Epoch:  32  Training Loss:  0.190261873625  Encoder Loss:  0.221021425417  Decoder Loss:  0.159502321881  Training Time:  1924.0\n",
      "Epoch:  33  Training Loss:  0.1890342564  Encoder Loss:  0.220109809871  Decoder Loss:  0.157958702534  Training Time:  1925.0\n",
      "Epoch:  34  Training Loss:  0.187578002988  Encoder Loss:  0.218532047549  Decoder Loss:  0.156623958747  Training Time:  1925.0\n",
      "Epoch:  35  Training Loss:  0.18584333843  Encoder Loss:  0.216652536464  Decoder Loss:  0.155034140494  Training Time:  1926.0\n",
      "Epoch:  36  Training Loss:  0.184622529871  Encoder Loss:  0.215687538973  Decoder Loss:  0.153557520857  Training Time:  1926.0\n",
      "Epoch:  37  Training Loss:  0.183466582343  Encoder Loss:  0.214456496917  Decoder Loss:  0.152476667843  Training Time:  1926.0\n",
      "Epoch:  38  Training Loss:  0.182539541393  Encoder Loss:  0.213704497205  Decoder Loss:  0.151374585484  Training Time:  1925.0\n",
      "Epoch:  39  Training Loss:  0.180924455909  Encoder Loss:  0.211928258235  Decoder Loss:  0.149920653675  Training Time:  1925.0\n",
      "Epoch:  40  Training Loss:  0.179592737815  Encoder Loss:  0.21023934911  Decoder Loss:  0.148946126479  Training Time:  1925.0\n",
      "Epoch:  41  Training Loss:  0.178761020017  Encoder Loss:  0.209770065769  Decoder Loss:  0.147751974314  Training Time:  1925.0\n",
      "Epoch:  42  Training Loss:  0.177472241332  Encoder Loss:  0.208153530224  Decoder Loss:  0.146790952306  Training Time:  1925.0\n",
      "Epoch:  43  Training Loss:  0.176612304022  Encoder Loss:  0.207305740487  Decoder Loss:  0.145918867271  Training Time:  1925.0\n",
      "Epoch:  44  Training Loss:  0.175464310685  Encoder Loss:  0.206127624186  Decoder Loss:  0.144800997373  Training Time:  1925.0\n",
      "Epoch:  45  Training Loss:  0.174790403355  Encoder Loss:  0.205488154024  Decoder Loss:  0.144092652787  Training Time:  1925.0\n",
      "Epoch:  46  Training Loss:  0.173551479088  Encoder Loss:  0.203888915754  Decoder Loss:  0.143214042128  Training Time:  1926.0\n",
      "Epoch:  47  Training Loss:  0.172840852066  Encoder Loss:  0.203318155443  Decoder Loss:  0.142363548688  Training Time:  1926.0\n",
      "Epoch:  48  Training Loss:  0.172246493812  Encoder Loss:  0.202763995196  Decoder Loss:  0.141728992232  Training Time:  1926.0\n",
      "Epoch:  49  Training Loss:  0.170805116849  Encoder Loss:  0.20068635897  Decoder Loss:  0.140923874683  Training Time:  1926.0\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "Mstr_LOSS = [np.inf] #np.zeros(EPOCH_UP)\n",
    "Slv1_LOSS = [np.inf] #np.zeros(EPOCH_UP)\n",
    "LOSS = [np.inf] #np.zeros(EPOCH_UP)\n",
    "total_loss = 0\n",
    "total_Slv1_loss = 0\n",
    "total_Mstr_loss = 0\n",
    "for ep in range(EPOCH_UP):\n",
    "    # Training\n",
    "    sp_list = [i for i in range(NUM_SPEAKER)]\n",
    "    random.shuffle(sp_list)    \n",
    "    sp_idx = 0\n",
    "    BATCH_idx = 0 # to record starting point of batch we want to extract\n",
    "    TIME_idx  = 0\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        mix, t1, t2, sequence, sp_idx, BATCH_idx, TIME_idx = get_batch_train(\n",
    "            mix_train, target1, target2, BATCH_idx, TIME_idx, sp_idx, BATCH_SIZE, TIME_STEP, INPUT_SIZE, sp_list, \n",
    "            sequence_length, longest=200, window=WINDOW)     \n",
    "        \n",
    "        # break\n",
    "        if sp_idx == NUM_SPEAKER:\n",
    "            break\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        model.sequence_length = sequence\n",
    "        # stochastic gradient descent\n",
    "        _, Slv1_loss, Mstr_loss, loss = sess.run(\n",
    "            [model.train_op, model.Slv1_loss, model.Mstr_loss, model.loss],\n",
    "            feed_dict\n",
    "            )\n",
    "        if math.isnan(loss):\n",
    "            break\n",
    "        #model.init_state_assign()\n",
    "        \n",
    "        # cost\n",
    "        total_loss += loss\n",
    "        total_Slv1_loss += Slv1_loss\n",
    "        total_Mstr_loss += Mstr_loss\n",
    "    end = time.time()  \n",
    "    train_time = end-start\n",
    "    \n",
    "    TOTAL_SIZE = 0\n",
    "    for key in sequence_length.keys():\n",
    "        TOTAL_SIZE += sum(sequence_length[key])\n",
    "        \n",
    "    LOSS.append(total_loss/(TOTAL_SIZE))\n",
    "    Slv1_LOSS.append(total_Slv1_loss/(TOTAL_SIZE))\n",
    "    Mstr_LOSS.append(total_Mstr_loss/(TOTAL_SIZE))\n",
    "    total_loss = 0\n",
    "    total_Mstr_loss = 0\n",
    "    total_Slv1_loss = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    print 'Epoch: ', ep, ' Training Loss: ', LOSS[-1], ' Encoder Loss: ', Slv1_LOSS[-1], ' Decoder Loss: ', Mstr_LOSS[-1], ' Training Time: ', math.ceil(train_time)\n",
    "    \n",
    "    if Mstr_LOSS[-2]-Mstr_LOSS[-1] < 0 and ep > 15: #or ep_idx >= 3:\n",
    "        break\n",
    "                        \n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "#save_path = saver.save(sess, \"./Models/recall\"+str(NUM_SPEAKER)+\".ckpt\")\n",
    "save_path = saver.save(sess, \"./Models/\"+model_name+str(NUM_SPEAKER)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total parameters: ', 36279978)\n"
     ]
    }
   ],
   "source": [
    "total_para = 0\n",
    "for var in tf.trainable_variables():\n",
    "    shape = var.get_shape()\n",
    "    tmp = 1\n",
    "    for dim in shape:\n",
    "        tmp *= dim.value\n",
    "    total_para += tmp\n",
    "print('Total parameters: ', total_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD9CAYAAABHnDf0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3EVEUpDCXUQkBSncRURSkMJdRCQF/X/aI3bGkEVAkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65949c1250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718173241229\n",
      "0.508680267376\n",
      "0.444484774359\n",
      "0.401199498388\n",
      "0.368379492539\n",
      "0.346231710363\n",
      "0.328894733726\n",
      "0.311964064407\n",
      "0.29925784475\n",
      "0.2882411332\n",
      "0.276169978747\n",
      "0.268303855995\n",
      "0.26071817505\n",
      "0.253588174706\n",
      "0.247428937323\n",
      "0.242075530508\n",
      "0.236745381196\n",
      "0.232119155639\n",
      "0.228021871833\n",
      "0.223273319119\n",
      "0.220269702788\n",
      "0.216535057\n",
      "0.213259171143\n",
      "0.21009205676\n",
      "0.207607335863\n",
      "0.205222182725\n",
      "0.202471030626\n",
      "0.200289303274\n",
      "0.19857535262\n",
      "0.196060161182\n",
      "0.194291172191\n",
      "0.19223276106\n",
      "0.190261873625\n",
      "0.1890342564\n",
      "0.187578002988\n",
      "0.18584333843\n",
      "0.184622529871\n",
      "0.183466582343\n",
      "0.182539541393\n",
      "0.180924455909\n",
      "0.179592737815\n",
      "0.178761020017\n",
      "0.177472241332\n",
      "0.176612304022\n",
      "0.175464310685\n",
      "0.174790403355\n",
      "0.173551479088\n",
      "0.172840852066\n",
      "0.172246493812\n",
      "0.170805116849\n",
      "--------------------\n",
      "0.668369286166\n",
      "0.514038774966\n",
      "0.457256689325\n",
      "0.417912168387\n",
      "0.38654420524\n",
      "0.366814972482\n",
      "0.351474965071\n",
      "0.335546499204\n",
      "0.324084508392\n",
      "0.313772858575\n",
      "0.302607654221\n",
      "0.296109026263\n",
      "0.289328194404\n",
      "0.282345753718\n",
      "0.276864355392\n",
      "0.272187215654\n",
      "0.266708502896\n",
      "0.262360886885\n",
      "0.258456207065\n",
      "0.253814209152\n",
      "0.251055198472\n",
      "0.247476930071\n",
      "0.24415578122\n",
      "0.241016011455\n",
      "0.238600981356\n",
      "0.236348639287\n",
      "0.233440752528\n",
      "0.231488375368\n",
      "0.229832864295\n",
      "0.227180842508\n",
      "0.225181903661\n",
      "0.223211557313\n",
      "0.221021425417\n",
      "0.220109809871\n",
      "0.218532047549\n",
      "0.216652536464\n",
      "0.215687538973\n",
      "0.214456496917\n",
      "0.213704497205\n",
      "0.211928258235\n",
      "0.21023934911\n",
      "0.209770065769\n",
      "0.208153530224\n",
      "0.207305740487\n",
      "0.206127624186\n",
      "0.205488154024\n",
      "0.203888915754\n",
      "0.203318155443\n",
      "0.202763995196\n",
      "0.20068635897\n",
      "--------------------\n",
      "0.767977197628\n",
      "0.503321760745\n",
      "0.431712858878\n",
      "0.384486828095\n",
      "0.350214780101\n",
      "0.325648448494\n",
      "0.306314502212\n",
      "0.288381630192\n",
      "0.274431180549\n",
      "0.262709406951\n",
      "0.249732303302\n",
      "0.240498685344\n",
      "0.232108155655\n",
      "0.224830595674\n",
      "0.217993519164\n",
      "0.211963845195\n",
      "0.206782259374\n",
      "0.201877425179\n",
      "0.197587536162\n",
      "0.192732429\n",
      "0.18948420692\n",
      "0.185593183908\n",
      "0.182362560971\n",
      "0.179168102087\n",
      "0.176613690443\n",
      "0.174095725929\n",
      "0.171501308588\n",
      "0.169090231336\n",
      "0.167317840706\n",
      "0.164939479943\n",
      "0.163400441008\n",
      "0.161253965105\n",
      "0.159502321881\n",
      "0.157958702534\n",
      "0.156623958747\n",
      "0.155034140494\n",
      "0.153557520857\n",
      "0.152476667843\n",
      "0.151374585484\n",
      "0.149920653675\n",
      "0.148946126479\n",
      "0.147751974314\n",
      "0.146790952306\n",
      "0.145918867271\n",
      "0.144800997373\n",
      "0.144092652787\n",
      "0.143214042128\n",
      "0.142363548688\n",
      "0.141728992232\n",
      "0.140923874683\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "#ax.plot(LOSS)\n",
    "ax.plot(Slv1_LOSS)\n",
    "ax.plot(Mstr_LOSS)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "for idx in LOSS[1:]:\n",
    "    print idx\n",
    "print(\"--------------------\")\n",
    "for idx in Slv1_LOSS[1:]:\n",
    "    print idx\n",
    "print(\"--------------------\")\n",
    "for idx in Mstr_LOSS[1:]:\n",
    "    print idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 1: Merge -5 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c501ca7f9001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmix_test2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjdx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_-5dB/Mix/', '../mann/Test1_merge_-5dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_-5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.213565524975\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_-5dB/Mix/', '../mann/Test1_merge_-5dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_-5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 20, 20))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 20, 20))\n",
    "total_c1 = np.zeros((1, 800))\n",
    "total_c2 = np.zeros((1, 800))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Slv1_pred1, model.Slv1_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        #total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        #total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        #total_c1 = np.append(total_c1, attention[2], axis=0)\n",
    "        #total_c2 = np.append(total_c2, attention[3], axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "#total_a1 = total_a1[:, :, 20:]\n",
    "#total_a2 = total_a2[:, :, 20:]\n",
    "#total_c1 = total_c1[1:, :]\n",
    "#total_c2 = total_c2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "#total_c1 = np.transpose(total_c1)\n",
    "#total_c2 = np.transpose(total_c2)\n",
    "            \n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test1/attention_a1.csv', np.reshape(total_a1, [50, 20*960]), delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test1/attention_a2.csv', np.reshape(total_a2, [50, 20*960]), delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test1/attention_c1.csv', total_c1, delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test1/attention_c2.csv', total_c2, delimiter=\",\")\n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 2: Merge -5dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  22538.4592972\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_-5dB/Mix/', '../mann/Test2_merge_-5dB/Target1/', \n",
    "                                                               '../mann/Test2_merge_-5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test2_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_-5dB/attention_a1_long_sequence.csv', total_a1[0], delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_-5dB/attention_a2_long_sequence.csv', total_a2[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.388802303173\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_-5dB/Mix/', '../mann/Test2_merge_-5dB/Target1/', \n",
    "                                                                      '../mann/Test2_merge_-5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 20, 20))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 20, 20))\n",
    "total_c1 = np.zeros((1, 800))\n",
    "total_c2 = np.zeros((1, 800))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        #total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        #total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        #total_c1 = np.append(total_c1, attention[2], axis=0)\n",
    "        #total_c2 = np.append(total_c2, attention[3], axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "#total_a1 = total_a1[:, :, 20:]\n",
    "#total_a2 = total_a2[:, :, 20:]\n",
    "#total_c1 = total_c1[1:, :]\n",
    "#total_c2 = total_c2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "#total_c1 = np.transpose(total_c1)\n",
    "#total_c2 = np.transpose(total_c2)\n",
    "            \n",
    "np.savetxt('./Prediction1/Test2_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_-5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test2/attention_a1.csv', np.reshape(total_a1, [50, 20*960]), delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test2/attention_a2.csv', np.reshape(total_a2, [50, 20*960]), delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test2/attention_c1.csv', total_c1, delimiter=\",\")\n",
    "#np.savetxt('./Prediction1/Test2/attention_c2.csv', total_c2, delimiter=\",\")\n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 1: Merge 0 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  13783.1533566\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_0dB/Mix/', '../mann/Test1_merge_0dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_0dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1_merge_0dB/attention_a1_long_sequence.csv', total_a1[0], delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1_merge_0dB/attention_a2_long_sequence.csv', total_a2[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.10618849497\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_0dB/Mix/', '../mann/Test1_merge_0dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_0dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')\n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 2: Merge 0 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  18847.9772606\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_0dB/Mix/', '../mann/Test2_merge_0dB/Target1/', \n",
    "                                                               '../mann/Test2_merge_0dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Dec_loss, model.Dec_pred1, model.Dec_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test2_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_0dB/attention_a1_long_sequence.csv', total_a1[0], delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_0dB/attention_a2_long_sequence.csv', total_a2[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.180737709893\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_0dB/Mix/', '../mann/Test2_merge_0dB/Target1/', \n",
    "                                                               '../mann/Test2_merge_0dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "total_enc_loss = 0\n",
    "total_dec_loss = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')\n",
    "\n",
    "np.savetxt('./Prediction1/Test2_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_0dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 1: Merge 5 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  11156.1681662\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_5dB/Mix/', '../mann/Test1_merge_5dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Dec_loss, model.Dec_pred1, model.Dec_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1_merge_5dB/attention_a1_long_sequence.csv', total_a1[0], delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1_merge_5dB/attention_a2_long_sequence.csv', total_a2[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.0619891352256\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test1_merge_5dB/Mix/', '../mann/Test1_merge_5dB/Target1/', \n",
    "                                                               '../mann/Test1_merge_5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')\n",
    "\n",
    "np.savetxt('./Prediction1/Test1_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test 2: Merge 5 dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  16550.0468979\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_5dB/Mix/', '../mann/Test2_merge_5dB/Target1/', \n",
    "                                                               '../mann/Test2_merge_5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "total_a2 = np.zeros((BATCH_SIZE, 539, 539))\n",
    "speaker = mix_test2.keys()\n",
    "speaker.sort()\n",
    "for idx in speaker:\n",
    "    sentence = mix_test2[idx].keys()\n",
    "    sentence.sort()\n",
    "    for jdx in sentence:\n",
    "        model.sequence_length = [sequence_length[idx][jdx]]\n",
    "        TOTAL_SIZE += sequence_length[idx][jdx][0]\n",
    "        mix, t1, t2 = mix_test2[idx][jdx], t1_test2[idx][jdx], t2_test2[idx][jdx]\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.x  : mix,\n",
    "            model.y1 : t1, \n",
    "            model.y2 : t2\n",
    "        }\n",
    "        loss, pred1, pred2, attention = sess.run(\n",
    "            [model.Dec_loss, model.Dec_pred1, model.Dec_pred2, model.attention], \n",
    "            feed_dict)\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=2)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=2)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "print 'The cost of model: ', total_loss/TOTAL_SIZE\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = total_a1[:, :, 539:]\n",
    "total_a2 = total_a2[:, :, 539:]\n",
    "\n",
    "np.savetxt('./Prediction1/Test2_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'_long_sequence.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_5dB/attention_a1_long_sequence.csv', total_a1[0], delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2_merge_5dB/attention_a2_long_sequence.csv', total_a2[0], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.09385864498\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, sequence_length = get_data_test('../mann/Test2_merge_5dB/Mix/', '../mann/Test2_merge_5dB/Target1/', \n",
    "                                                               '../mann/Test2_merge_5dB/Target2/', NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "\n",
    "total_loss  = 0\n",
    "total_enc_loss = 0\n",
    "total_dec_loss = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')\n",
    "\n",
    "np.savetxt('./Prediction1/Test2_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2_merge_5dB/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.216931670586\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test1, t1_test1, t2_test1, order, sequence_length = get_data_test('../mann/Test1/Mix/', '../mann/Test1/Target1/', '../mann/Test1/Target2/', \n",
    "                                                                      NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "total_loss  = 0\n",
    "total_enc_loss = 0\n",
    "total_dec_loss = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((1, 800))\n",
    "total_a2 = np.zeros((1, 800))\n",
    "total_c1 = np.zeros((1, 800))\n",
    "total_c2 = np.zeros((1, 800))\n",
    "sp_list     = mix_test1.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test1[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test1[sp_list[speaker]], t1_test1[sp_list[speaker]], t2_test1[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2, attention = sess.run(\n",
    "                [model.Dec_loss, model.Dec_pred1, model.Dec_pred2, model.attention],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, enc_loss, dec_loss, pred1, pred2, attention = sess.run(\n",
    "                [model.Dec_loss, model.Enc_loss, model.Dec_loss, model.Dec_pred1, model.Dec_pred2, model.attention],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=0)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=0)\n",
    "        total_c1 = np.append(total_c1, attention[2], axis=0)\n",
    "        total_c2 = np.append(total_c2, attention[3], axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "        total_enc_loss += enc_loss\n",
    "        total_dec_loss += dec_loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_a1 = total_a1[1:, :]\n",
    "total_a2 = total_a2[1:, :]\n",
    "total_c1 = total_c1[1:, :]\n",
    "total_c2 = total_c2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = np.transpose(total_a1)\n",
    "total_a2 = np.transpose(total_a2)\n",
    "total_c1 = np.transpose(total_c1)\n",
    "total_c2 = np.transpose(total_c2)\n",
    "            \n",
    "np.savetxt('./Prediction1/Test1/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test1/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1/attention_a1.csv', total_a1, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1/attention_a2.csv', total_a2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1/attention_c1.csv', total_c1, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test1/attention_c2.csv', total_c2, delimiter=\",\")\n",
    "\n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of model:  0.352608662782\n",
      "----------End----------\n"
     ]
    }
   ],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, order, sequence_length = get_data_test('../mann/Test2/Mix/', '../mann/Test2/Target1/', '../mann/Test2/Target2/', \n",
    "                                                                      NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)\n",
    "total_loss  = 0\n",
    "total_enc_loss = 0\n",
    "total_dec_loss = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "total_a1 = np.zeros((1, 800))\n",
    "total_a2 = np.zeros((1, 800))\n",
    "total_c1 = np.zeros((1, 800))\n",
    "total_c2 = np.zeros((1, 800))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            loss, pred1, pred2 = sess.run(\n",
    "                [model.Mstr_loss, model.Mstr_pred1, model.Mstr_pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        total_a1 = np.append(total_a1, attention[0], axis=0)\n",
    "        total_a2 = np.append(total_a2, attention[1], axis=0)\n",
    "        total_c1 = np.append(total_c1, attention[2], axis=0)\n",
    "        total_c2 = np.append(total_c2, attention[3], axis=0)\n",
    "        # cost\n",
    "        total_loss += loss\n",
    "            \n",
    "total_loss = total_loss/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_loss\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_a1 = total_a1[1:, :]\n",
    "total_a2 = total_a2[1:, :]\n",
    "total_c1 = total_c1[1:, :]\n",
    "total_c2 = total_c2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "total_a1 = np.transpose(total_a1)\n",
    "total_a2 = np.transpose(total_a2)\n",
    "total_c1 = np.transpose(total_c1)\n",
    "total_c2 = np.transpose(total_c2)\n",
    "            \n",
    "np.savetxt('./Prediction1/Test2/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('./Prediction2/Test2/'+model_name+'_'+str(NUM_SPEAKER)+'.csv', total_pred2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2/attention_a1.csv', total_a1, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2/attention_a2.csv', total_a2, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2/attention_c1.csv', total_c1, delimiter=\",\")\n",
    "np.savetxt('./Prediction1/Test2/attention_c2.csv', total_c2, delimiter=\",\")\n",
    "\n",
    "print('----------End----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
